import tensorflow as tf
import numpy as np
import sys
from prepare_non_windowed_data import getEncoderDecoderData
from BatchIterator import *

NUM_ENC_FEATURES = 5
NUM_DEC_FEATURES = 5
NUM_LABELS = 12
MAX_STEPS = 700
BATCH_SIZE = 256
NUM_EPOCHS = 1000
LR = 0.005
THRESHOLD = 0.5

SEED = 123 # set graph-level seed to make the random sequences generated by all ops be repeatable across sessions
tf.set_random_seed(SEED)
np.random.seed(SEED)

def encoderDecoderModel(encoderInput, \
        encoderOutput, \
        decoderInput, \
        decoderOutput, \
        seqlen, \
        state_size, \
        decoder_filters, \
        num_dec_features, \
        num_labels):

    num_samples, num_enc_steps = tf.shape(encoderInput)[0], tf.shape(encoderInput)[1]
    num_dec_steps = tf.shape(decoderInput)[1]

    ## Encoder
    cell = tf.contrib.rnn.LSTMCell(state_size,state_is_tuple=True)
    encoderOutput = tf.concat([tf.zeros([num_samples,1]), encoderOutput[:,:-1]], axis=1)
    rnn_inputs = tf.concat([tf.expand_dims(encoderOutput, axis=-1), encoderInput], axis=2)
    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen, dtype=tf.float32)

    #---------- Block start ---------#
#    rnn_outputs_shape = tf.shape(rnn_outputs)
#    rnn_outputs = tf.reshape(rnn_outputs, [-1, rnn_outputs_shape[-1]])
#
#    # Linear layer over encoder outputs
#    W_enc = tf.get_variable('Wenc', [state_size, num_labels], dtype=tf.float32)
#    b_enc = tf.get_variable('Benc', [1, num_labels], dtype=tf.float32)
#    encoder_out = tf.matmul(rnn_outputs, W_enc) + b_enc
#
#    ## Decoder
#    W_dec_hl = tf.get_variable('Wdechl', [num_dec_features, decoder_filters], dtype=tf.float32)
#    b_dec_hl = tf.get_variable('Bdechl', [decoder_filters], dtype=tf.float32)
#    W_dec_labels = tf.get_variable('Wdeclbl', [decoder_filters], dtype=tf.float32)
#    b_dec_labels = tf.get_variable('bdeclbl', [1], dtype=tf.float32)
#
#    decoderInput = tf.reshape(decoderInput, [-1, num_labels, num_dec_features])
#    decoderOutput = tf.reshape(decoderOutput, [-1, num_labels])
#
#    decoder_hl = tf.tensordot(decoderInput, W_dec_hl, axes=[[2],[0]]) + b_dec_hl
#    #decoder_hl = tf.reshape(decoder_hl, [-1, num_labels, decoder_filters])
#    decoder_out = tf.tensordot(decoder_hl, W_dec_labels, axes=[[2],[0]]) + b_dec_labels
#
#    logits = encoder_out + decoder_out
#    logits = tf.Print(logits, [tf.shape(logits)])
    #---------- Block End ---------#

    #---------- Block Start ---------#
    encoder_out = tf.layers.dense(rnn_outputs, num_labels, activation=tf.nn.relu)

    decoder_h1 = tf.layers.dense(decoderInput, decoder_filters, activation=tf.nn.relu)
    decoder_out = tf.layers.dense(decoder_h1, 1)
    decoder_out = tf.squeeze(decoder_out, axis=3)

    logits = encoder_out + decoder_out
    #---------- Block End ---------#

    seqlen_mask = tf.slice(tf.gather(lower_triangular_ones, seqlen-1), [0, 0], [num_samples, num_dec_steps])
    seqlen_mask = tf.expand_dims(seqlen_mask, axis=-1)
    predictions = tf.nn.sigmoid(logits)
    decoderOutputLoss = -(decoderOutput * tf.log(predictions+1e-9) \
            + (1.0-decoderOutput) * tf.log(1-predictions+1e-9))
#    decoderOutputLoss = tf.reshape(decoderOutputLoss, [num_samples, num_dec_steps, num_labels])
    decoderOutputLoss *= seqlen_mask
    #decoderOutputLoss = tf.divide(tf.reduce_sum(decoderOutputLoss), tf.reduce_sum(seqlen_mask))
    decoderOutputLoss = tf.reduce_sum(decoderOutputLoss)
#    predictions = tf.reshape(predictions, shape=[num_samples, num_dec_steps, num_labels])
    predictions *= seqlen_mask

    return decoderOutputLoss, seqlen_mask, predictions


X_enc = tf.placeholder(shape=[None, None, NUM_ENC_FEATURES],
        dtype=tf.float32) #Features of history data (encoder input)
Y_enc = tf.placeholder(shape=[None, None], 
        dtype=tf.float32) #Labels of history data (encoder output) 
X_dec = tf.placeholder(shape=[None, None, NUM_LABELS, NUM_DEC_FEATURES],
        dtype=tf.float32) #Features of next 24 hours, sampled by 5 mins (decoder input) 
Y_dec = tf.placeholder(shape=[None, None, NUM_LABELS],
        dtype=tf.float32) #Labels of next 24 hours, sampled by 5 mins (decoder output)
seqlen = tf.placeholder(tf.int32, shape=[None]) #List of lengths of each sequence
lower_triangular_ones = tf.constant(np.tril(np.ones([MAX_STEPS,MAX_STEPS])),dtype=tf.float32)


def train(encoderInput, encoderOutput, decoderInput, decoderOutput):

    decoderOutputLoss, seqlen_mask, predictions = \
            encoderDecoderModel(X_enc, Y_enc, X_dec, Y_dec, seqlen, \
            state_size=32, decoder_filters=3, \
            num_dec_features=NUM_DEC_FEATURES, num_labels=NUM_LABELS)

    correct = tf.cast(tf.equal(tf.cast((predictions >= THRESHOLD),'float32'), Y_dec), 'float32')
    correct_pos = tf.multiply(correct, Y_dec)
    correct_pos = tf.multiply(correct_pos, seqlen_mask)
    total_pos = Y_dec
    total_neg = tf.multiply((1.0-Y_dec), seqlen_mask)
    correct_neg = tf.multiply(correct, (1.0-Y_dec))
    correct_neg = tf.multiply(correct_neg, seqlen_mask)
    accuracy_pos = tf.divide(tf.reduce_sum(tf.cast(correct_pos, 'float32')),
            tf.cast(tf.reduce_sum(total_pos), 'float32'))
    accuracy_neg = tf.divide(tf.reduce_sum(tf.cast(correct_neg, 'float32')),
            tf.cast(tf.reduce_sum(total_neg), 'float32'))

    train_variables = tf.trainable_variables()
    print(map(lambda x: x.op.name, train_variables))

    train_op = tf.train.AdamOptimizer(learning_rate=LR).minimize(decoderOutputLoss, var_list=train_variables)

    # run

    # Add ops to save and restore all the variables.
    saver = tf.train.Saver()

    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.33, allow_growth=True)
    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options))

    sess.run(tf.global_variables_initializer())

    ## Train
    encoderInputItr = EncoderInputIterator(encoderInput)
    encoderOutputItr = EncoderOutputIterator(encoderOutput)
    decoderInputItr = DecoderInputIterator(decoderInput)
    decoderOutputItr = DecoderOutputIterator(decoderOutput)
    num_batches = encoderInputItr.size / BATCH_SIZE + (encoderInputItr.size%BATCH_SIZE>0)
    for epoch_no in range(NUM_EPOCHS):
        epoch_loss = 0.0
        for batch_no in range(num_batches):
            encoderInputBch, encoderInputBchLens = encoderInputItr.batch(BATCH_SIZE)
            encoderOutputBch, encoderOutputBchLens = encoderOutputItr.batch(BATCH_SIZE)
            decoderInputBch, decoderInputBchLens = decoderInputItr.batch(BATCH_SIZE)
            decoderOutputBch, decoderOutputBchLens = decoderOutputItr.batch(BATCH_SIZE)
            #print('Epoch: {}, Batch: {},'.format(epoch_no, batch_no))
            decoderOutputLossRet, _= sess.run([decoderOutputLoss,train_op],
                    feed_dict={X_enc:encoderInputBch,
                        Y_enc:encoderOutputBch,
                        X_dec:decoderInputBch,
                        Y_dec:decoderOutputBch,
                        seqlen:encoderInputBchLens})
            epoch_loss += decoderOutputLossRet
            #print('Epoch:', epoch_no, 'Batch:', batch_no, 'loss:' , decoderOutputLossRet)

        if epoch_no%2==0:
            print('Epoch', epoch_no, 'completed, loss:', epoch_loss)
            encoderInputEvalItr = EncoderInputIterator(encoderInput)
            encoderOutputEvalItr = EncoderOutputIterator(encoderOutput)
            decoderInputEvalItr = DecoderInputIterator(decoderInput)
            decoderOutputEvalItr = DecoderOutputIterator(decoderOutput)
            correct_ones, correct_zeros, total_ones, total_zeros = 0, 0, 0, 0
            for batch_no in range(num_batches):
                encoderInputBch, encoderInputBchLens = encoderInputEvalItr.batch(BATCH_SIZE)
                encoderOutputBch, encoderOutputBchLens = encoderOutputEvalItr.batch(BATCH_SIZE)
                decoderInputBch, decoderInputBchLens = decoderInputEvalItr.batch(BATCH_SIZE)
                decoderOutputBch, decoderOutputBchLens = decoderOutputEvalItr.batch(BATCH_SIZE)
                correct_pos_ret, correct_neg_ret, total_pos_ret, total_neg_ret, \
                                =  sess.run([correct_pos, correct_neg, total_pos, total_neg],
                                        feed_dict={X_enc:encoderInputBch,
                                        Y_enc:encoderOutputBch,
                                        X_dec:decoderInputBch,
                                        Y_dec:decoderOutputBch,
                                        seqlen:encoderInputBchLens})
                correct_ones += np.sum(correct_pos_ret)
                correct_zeros += np.sum(correct_neg_ret)
                total_ones += np.sum(total_pos_ret)
                total_zeros += np.sum(total_neg_ret)

            print('Accuracy 1s:', correct_ones*1.0/total_ones)
            print('Accuracy 0s:', correct_zeros*1.0/total_zeros)
            print('--------------------')


def main():
    filePath = sys.argv[1]
    encoderInput, encoderOutput, decoderInput, decoderOutput \
            = getEncoderDecoderData(filePath)
    labels_list_flat = list()
    for i in range(len(decoderOutput)):
        for j in range(len(decoderOutput[i])):
            labels_list_flat.append(decoderOutput[i][j])
    labels_list_flat = np.array(labels_list_flat)
    print(np.array(labels_list_flat).sum(axis=0))
    print((1.0-np.array(labels_list_flat)).sum(axis=0))
#    for i in range(len(decoderInput)):
#        for j in range(len(decoderInput[i])):
#            decoderInput[i][j] = decoderInput[i][j][50:50+NUM_LABELS]
#            decoderOutput[i][j] = decoderOutput[i][j][50:50+NUM_LABELS]

    train(encoderInput, encoderOutput, decoderInput, decoderOutput)

if __name__ == '__main__':
    main()
