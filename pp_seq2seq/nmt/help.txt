pratham@blossom:~/Forecasting/nmt$ python -m nmt.nmt --help
usage: nmt.py [-h] [--num_units NUM_UNITS] [--num_layers NUM_LAYERS]
              [--num_encoder_layers NUM_ENCODER_LAYERS]
              [--num_decoder_layers NUM_DECODER_LAYERS]
              [--encoder_type ENCODER_TYPE] [--residual [RESIDUAL]]
              [--time_major [TIME_MAJOR]]
              [--num_embeddings_partitions NUM_EMBEDDINGS_PARTITIONS]
              [--attention ATTENTION]
              [--attention_architecture ATTENTION_ARCHITECTURE]
              [--output_attention [OUTPUT_ATTENTION]]
              [--pass_hidden_state [PASS_HIDDEN_STATE]]
              [--optimizer OPTIMIZER] [--learning_rate LEARNING_RATE]
              [--warmup_steps WARMUP_STEPS] [--warmup_scheme WARMUP_SCHEME]
              [--decay_scheme DECAY_SCHEME]
              [--num_train_steps NUM_TRAIN_STEPS]
              [--colocate_gradients_with_ops [COLOCATE_GRADIENTS_WITH_OPS]]
              [--init_op INIT_OP] [--init_weight INIT_WEIGHT] [--src SRC]
              [--tgt TGT] [--train_prefix TRAIN_PREFIX]
              [--dev_prefix DEV_PREFIX] [--test_prefix TEST_PREFIX]
              [--out_dir OUT_DIR] [--vocab_prefix VOCAB_PREFIX]
              [--embed_prefix EMBED_PREFIX] [--sos SOS] [--eos EOS]
              [--share_vocab [SHARE_VOCAB]]
              [--check_special_token CHECK_SPECIAL_TOKEN]
              [--src_max_len SRC_MAX_LEN] [--tgt_max_len TGT_MAX_LEN]
              [--src_max_len_infer SRC_MAX_LEN_INFER]
              [--tgt_max_len_infer TGT_MAX_LEN_INFER] [--unit_type UNIT_TYPE]
              [--forget_bias FORGET_BIAS] [--dropout DROPOUT]
              [--max_gradient_norm MAX_GRADIENT_NORM]
              [--batch_size BATCH_SIZE] [--steps_per_stats STEPS_PER_STATS]
              [--max_train MAX_TRAIN] [--num_buckets NUM_BUCKETS]
              [--num_sampled_softmax NUM_SAMPLED_SOFTMAX]
              [--subword_option {,bpe,spm}]
              [--use_char_encode USE_CHAR_ENCODE] [--num_gpus NUM_GPUS]
              [--log_device_placement [LOG_DEVICE_PLACEMENT]]
              [--metrics METRICS]
              [--steps_per_external_eval STEPS_PER_EXTERNAL_EVAL]
              [--scope SCOPE] [--hparams_path HPARAMS_PATH]
              [--random_seed RANDOM_SEED]
              [--override_loaded_hparams [OVERRIDE_LOADED_HPARAMS]]
              [--num_keep_ckpts NUM_KEEP_CKPTS] [--avg_ckpts [AVG_CKPTS]]
              [--language_model [LANGUAGE_MODEL]] [--ckpt CKPT]
              [--inference_input_file INFERENCE_INPUT_FILE]
              [--inference_list INFERENCE_LIST]
              [--infer_batch_size INFER_BATCH_SIZE]
              [--inference_output_file INFERENCE_OUTPUT_FILE]
              [--inference_ref_file INFERENCE_REF_FILE]
              [--infer_mode {greedy,sample,beam_search}]
              [--beam_width BEAM_WIDTH]
              [--length_penalty_weight LENGTH_PENALTY_WEIGHT]
              [--coverage_penalty_weight COVERAGE_PENALTY_WEIGHT]
              [--sampling_temperature SAMPLING_TEMPERATURE]
              [--num_translations_per_input NUM_TRANSLATIONS_PER_INPUT]
              [--jobid JOBID] [--num_workers NUM_WORKERS]
              [--num_inter_threads NUM_INTER_THREADS]
              [--num_intra_threads NUM_INTRA_THREADS]

optional arguments:
  -h, --help            show this help message and exit
  --num_units NUM_UNITS
                        Network size.
  --num_layers NUM_LAYERS
                        Network depth.
  --num_encoder_layers NUM_ENCODER_LAYERS
                        Encoder depth, equal to num_layers if None.
  --num_decoder_layers NUM_DECODER_LAYERS
                        Decoder depth, equal to num_layers if None.
  --encoder_type ENCODER_TYPE
                        uni | bi | gnmt. For bi, we build num_encoder_layers/2
                        bi-directional layers. For gnmt, we build 1 bi-
                        directional layer, and (num_encoder_layers - 1) uni-
                        directional layers.
  --residual [RESIDUAL]
                        Whether to add residual connections.
  --time_major [TIME_MAJOR]
                        Whether to use time-major mode for dynamic RNN.
  --num_embeddings_partitions NUM_EMBEDDINGS_PARTITIONS
                        Number of partitions for embedding vars.
  --attention ATTENTION
                        luong | scaled_luong | bahdanau | normed_bahdanau or
                        set to "" for no attention
  --attention_architecture ATTENTION_ARCHITECTURE
                        standard | gnmt | gnmt_v2. standard: use top layer to
                        compute attention. gnmt: GNMT style of computing
                        attention, use previous bottom layer to compute
                        attention. gnmt_v2: similar to gnmt, but use current
                        bottom layer to compute attention.
  --output_attention [OUTPUT_ATTENTION]
                        Only used in standard attention_architecture. Whether
                        use attention as the cell output at each timestep. .
  --pass_hidden_state [PASS_HIDDEN_STATE]
                        Whether to pass encoder's hidden state to decoder when
                        using an attention based model.
  --optimizer OPTIMIZER
                        sgd | adam
  --learning_rate LEARNING_RATE
                        Learning rate. Adam: 0.001 | 0.0001
  --warmup_steps WARMUP_STEPS
                        How many steps we inverse-decay learning.
  --warmup_scheme WARMUP_SCHEME
                        How to warmup learning rates. Options include: t2t:
                        Tensor2Tensor's way, start with lr 100 times smaller,
                        then exponentiate until the specified lr.
  --decay_scheme DECAY_SCHEME
                        How we decay learning rate. Options include: luong234:
                        after 2/3 num train steps, we start halving the
                        learning rate for 4 times before finishing. luong5:
                        after 1/2 num train steps, we start halving the
                        learning rate for 5 times before finishing. luong10:
                        after 1/2 num train steps, we start halving the
                        learning rate for 10 times before finishing.
  --num_train_steps NUM_TRAIN_STEPS
                        Num steps to train.
  --colocate_gradients_with_ops [COLOCATE_GRADIENTS_WITH_OPS]
                        Whether try colocating gradients with corresponding op
  --init_op INIT_OP     uniform | glorot_normal | glorot_uniform
  --init_weight INIT_WEIGHT
                        for uniform init_op, initialize weights between
                        [-this, this].
  --src SRC             Source suffix, e.g., en.
  --tgt TGT             Target suffix, e.g., de.
  --train_prefix TRAIN_PREFIX
                        Train prefix, expect files with src/tgt suffixes.
  --dev_prefix DEV_PREFIX
                        Dev prefix, expect files with src/tgt suffixes.
  --test_prefix TEST_PREFIX
                        Test prefix, expect files with src/tgt suffixes.
  --out_dir OUT_DIR     Store log/model files.
  --vocab_prefix VOCAB_PREFIX
                        Vocab prefix, expect files with src/tgt suffixes.
  --embed_prefix EMBED_PREFIX
                        Pretrained embedding prefix, expect files with src/tgt
                        suffixes. The embedding files should be Glove formated
                        txt files.
  --sos SOS             Start-of-sentence symbol.
  --eos EOS             End-of-sentence symbol.
  --share_vocab [SHARE_VOCAB]
                        Whether to use the source vocab and embeddings for
                        both source and target.
  --check_special_token CHECK_SPECIAL_TOKEN
                        Whether check special sos, eos, unk tokens exist in
                        the vocab files.
  --src_max_len SRC_MAX_LEN
                        Max length of src sequences during training.
  --tgt_max_len TGT_MAX_LEN
                        Max length of tgt sequences during training.
  --src_max_len_infer SRC_MAX_LEN_INFER
                        Max length of src sequences during inference.
  --tgt_max_len_infer TGT_MAX_LEN_INFER
                        Max length of tgt sequences during inference. Also use
                        to restrict the maximum decoding length.
  --unit_type UNIT_TYPE
                        lstm | gru | layer_norm_lstm | nas
  --forget_bias FORGET_BIAS
                        Forget bias for BasicLSTMCell.
  --dropout DROPOUT     Dropout rate (not keep_prob)
  --max_gradient_norm MAX_GRADIENT_NORM
                        Clip gradients to this norm.
  --batch_size BATCH_SIZE
                        Batch size.
  --steps_per_stats STEPS_PER_STATS
                        How many training steps to do per stats logging.Save
                        checkpoint every 10x steps_per_stats
  --max_train MAX_TRAIN
                        Limit on the size of training data (0: no limit).
  --num_buckets NUM_BUCKETS
                        Put data into similar-length buckets.
  --num_sampled_softmax NUM_SAMPLED_SOFTMAX
                        Use sampled_softmax_loss if > 0.Otherwise, use full
                        softmax loss.
  --subword_option {,bpe,spm}
                        Set to bpe or spm to activate subword desegmentation.
  --use_char_encode USE_CHAR_ENCODE
                        Whether to split each word or bpe into character, and
                        then generate the word-level representation from the
                        character reprentation.
  --num_gpus NUM_GPUS   Number of gpus in each worker.
  --log_device_placement [LOG_DEVICE_PLACEMENT]
                        Debug GPU allocation.
  --metrics METRICS     Comma-separated list of evaluations metrics
                        (bleu,rouge,accuracy)
  --steps_per_external_eval STEPS_PER_EXTERNAL_EVAL
                        How many training steps to do per external evaluation.
                        Automatically set based on data if None.
  --scope SCOPE         scope to put variables under
  --hparams_path HPARAMS_PATH
                        Path to standard hparams json file that
                        overrideshparams values from FLAGS.
  --random_seed RANDOM_SEED
                        Random seed (>0, set a specific seed).
  --override_loaded_hparams [OVERRIDE_LOADED_HPARAMS]
                        Override loaded hparams with values specified
  --num_keep_ckpts NUM_KEEP_CKPTS
                        Max number of checkpoints to keep.
  --avg_ckpts [AVG_CKPTS]
                        Average the last N checkpoints for external
                        evaluation. N can be controlled by setting
                        --num_keep_ckpts.
  --language_model [LANGUAGE_MODEL]
                        True to train a language model, ignoring encoder
  --ckpt CKPT           Checkpoint file to load a model for inference.
  --inference_input_file INFERENCE_INPUT_FILE
                        Set to the text to decode.
  --inference_list INFERENCE_LIST
                        A comma-separated list of sentence indices (0-based)
                        to decode.
  --infer_batch_size INFER_BATCH_SIZE
                        Batch size for inference mode.
  --inference_output_file INFERENCE_OUTPUT_FILE
                        Output file to store decoding results.
  --inference_ref_file INFERENCE_REF_FILE
                        Reference file to compute evaluation scores (if
                        provided).
  --infer_mode {greedy,sample,beam_search}
                        Which type of decoder to use during inference.
  --beam_width BEAM_WIDTH
                        beam width when using beam search decoder. If 0
                        (default), use standard decoder with greedy helper.
  --length_penalty_weight LENGTH_PENALTY_WEIGHT
                        Length penalty for beam search.
  --coverage_penalty_weight COVERAGE_PENALTY_WEIGHT
                        Coverage penalty for beam search.
  --sampling_temperature SAMPLING_TEMPERATURE
                        Softmax sampling temperature for inference decoding,
                        0.0 means greedy decoding. This option is ignored when
                        using beam search.
  --num_translations_per_input NUM_TRANSLATIONS_PER_INPUT
                        Number of translations generated for each sentence.
                        This is only used for inference.
  --jobid JOBID         Task id of the worker.
  --num_workers NUM_WORKERS
                        Number of workers (inference only).
  --num_inter_threads NUM_INTER_THREADS
                        number of inter_op_parallelism_threads
  --num_intra_threads NUM_INTRA_THREADS
                        number of intra_op_parallelism_threads
